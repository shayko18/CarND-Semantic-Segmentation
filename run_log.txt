ubuntu@ip-172-31-13-28:~/CarND-Semantic-Segmentation$ python main.py
TensorFlow Version: 1.2.1
2017-08-14 10:34:52.014241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-08-14 10:34:52.014594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:
name: Tesla M60
major: 5 minor: 2 memoryClockRate (GHz) 1.1775
pciBusID 0000:00:1e.0
Total memory: 7.43GiB
Free memory: 7.36GiB
2017-08-14 10:34:52.014617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0
2017-08-14 10:34:52.014629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y
2017-08-14 10:34:52.014642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0)
2017-08-14 10:34:52.100092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0)
Default GPU Device: /gpu:0
2017-08-14 10:34:52.101137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0)
Tests Passed
Tests Passed
2017-08-14 10:34:52.257305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0)
2017-08-14 10:34:52.268067: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-08-14 10:34:52.268090: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 16 visible devices
2017-08-14 10:34:52.272236: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x3bd4550 executing computations on platform Host. Devices:
2017-08-14 10:34:52.272254: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): <undefined>, <undefined>
2017-08-14 10:34:52.272379: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices
2017-08-14 10:34:52.272394: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 16 visible devices
2017-08-14 10:34:52.274980: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x3e228e0 executing computations on platform CUDA. Devices:
2017-08-14 10:34:52.274997: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): Tesla M60, Compute Capability 5.2
Tests Passed
2017-08-14 10:34:52.475149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0)
Tests Passed
Tests Passed
2017-08-14 10:34:52.484170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0)
Epoch 1/15 ; k=0 ; Training Loss: 129.355
Epoch 1/15 ; k=10 ; Training Loss: 3.394
Epoch 1/15 ; k=20 ; Training Loss: 1.108
Epoch 1/15 ; k=30 ; Training Loss: 0.839
Epoch 1/15 ; k=40 ; Training Loss: 0.749
Epoch 1/15 ; k=50 ; Training Loss: 0.689
Epoch 1/15 ; k=60 ; Training Loss: 0.737
Epoch 1/15 ; k=70 ; Training Loss: 0.656
Epoch 1/15 ; k=80 ; Training Loss: 0.672
Epoch 1/15 ; k=90 ; Training Loss: 0.650
Epoch 1/15 ; k=100 ; Training Loss: 0.630
Epoch 1/15 ; k=110 ; Training Loss: 0.630
Epoch 1/15 ; k=120 ; Training Loss: 0.643
Epoch 1/15 ; k=130 ; Training Loss: 0.612
Epoch 1/15 ; k=140 ; Training Loss: 0.587
Epoch 1/15 ; k=150 ; Training Loss: 0.571
Epoch 1/15 ; k=160 ; Training Loss: 0.571
Epoch 1/15 ; k=170 ; Training Loss: 0.581
Epoch 1/15 ; k=180 ; Training Loss: 0.531
Epoch 1/15 ; k=190 ; Training Loss: 0.482
Epoch 1/15 ; k=200 ; Training Loss: 0.476
Epoch 1/15 ; k=210 ; Training Loss: 0.390
Epoch 1/15 ; k=220 ; Training Loss: 0.405
Epoch 1/15 ; k=230 ; Training Loss: 0.403
Epoch 1/15 ; k=240 ; Training Loss: 0.370
Epoch 1/15 ; k=250 ; Training Loss: 0.291
Epoch 1/15 ; k=260 ; Training Loss: 0.280
Epoch 1/15 ; k=270 ; Training Loss: 0.285
Epoch 1/15 ; k=280 ; Training Loss: 0.290
Epoch 2/15 ; k=0 ; Training Loss: 0.274
Epoch 2/15 ; k=10 ; Training Loss: 0.256
Epoch 2/15 ; k=20 ; Training Loss: 0.430
Epoch 2/15 ; k=30 ; Training Loss: 0.244
Epoch 2/15 ; k=40 ; Training Loss: 0.291
Epoch 2/15 ; k=50 ; Training Loss: 0.242
Epoch 2/15 ; k=60 ; Training Loss: 0.176
Epoch 2/15 ; k=70 ; Training Loss: 0.275
Epoch 2/15 ; k=80 ; Training Loss: 0.297
Epoch 2/15 ; k=90 ; Training Loss: 0.260
Epoch 2/15 ; k=100 ; Training Loss: 0.200
Epoch 2/15 ; k=110 ; Training Loss: 0.222
Epoch 2/15 ; k=120 ; Training Loss: 0.255
Epoch 2/15 ; k=130 ; Training Loss: 0.260
Epoch 2/15 ; k=140 ; Training Loss: 0.194
Epoch 2/15 ; k=150 ; Training Loss: 0.279
Epoch 2/15 ; k=160 ; Training Loss: 0.289
Epoch 2/15 ; k=170 ; Training Loss: 0.176
Epoch 2/15 ; k=180 ; Training Loss: 0.199
Epoch 2/15 ; k=190 ; Training Loss: 0.197
Epoch 2/15 ; k=200 ; Training Loss: 0.195
Epoch 2/15 ; k=210 ; Training Loss: 0.213
Epoch 2/15 ; k=220 ; Training Loss: 0.255
Epoch 2/15 ; k=230 ; Training Loss: 0.217
Epoch 2/15 ; k=240 ; Training Loss: 0.209
Epoch 2/15 ; k=250 ; Training Loss: 0.335
Epoch 2/15 ; k=260 ; Training Loss: 0.189
Epoch 2/15 ; k=270 ; Training Loss: 0.230
Epoch 2/15 ; k=280 ; Training Loss: 0.199
Epoch 3/15 ; k=0 ; Training Loss: 0.176
Epoch 3/15 ; k=10 ; Training Loss: 0.168
Epoch 3/15 ; k=20 ; Training Loss: 0.105
Epoch 3/15 ; k=30 ; Training Loss: 0.089
Epoch 3/15 ; k=40 ; Training Loss: 0.160
Epoch 3/15 ; k=50 ; Training Loss: 0.208
Epoch 3/15 ; k=60 ; Training Loss: 0.151
Epoch 3/15 ; k=70 ; Training Loss: 0.162
Epoch 3/15 ; k=80 ; Training Loss: 0.271
Epoch 3/15 ; k=90 ; Training Loss: 0.157
Epoch 3/15 ; k=100 ; Training Loss: 0.220
Epoch 3/15 ; k=110 ; Training Loss: 0.256
Epoch 3/15 ; k=120 ; Training Loss: 0.165
Epoch 3/15 ; k=130 ; Training Loss: 0.219
Epoch 3/15 ; k=140 ; Training Loss: 0.290
Epoch 3/15 ; k=150 ; Training Loss: 0.232
Epoch 3/15 ; k=160 ; Training Loss: 0.117
Epoch 3/15 ; k=170 ; Training Loss: 0.187
Epoch 3/15 ; k=180 ; Training Loss: 0.163
Epoch 3/15 ; k=190 ; Training Loss: 0.182
Epoch 3/15 ; k=200 ; Training Loss: 0.202
Epoch 3/15 ; k=210 ; Training Loss: 0.088
Epoch 3/15 ; k=220 ; Training Loss: 0.212
Epoch 3/15 ; k=230 ; Training Loss: 0.185
Epoch 3/15 ; k=240 ; Training Loss: 0.200
Epoch 3/15 ; k=250 ; Training Loss: 0.123
Epoch 3/15 ; k=260 ; Training Loss: 0.155
Epoch 3/15 ; k=270 ; Training Loss: 0.228
Epoch 3/15 ; k=280 ; Training Loss: 0.170
Epoch 4/15 ; k=0 ; Training Loss: 0.137
Epoch 4/15 ; k=10 ; Training Loss: 0.204
Epoch 4/15 ; k=20 ; Training Loss: 0.138
Epoch 4/15 ; k=30 ; Training Loss: 0.204
Epoch 4/15 ; k=40 ; Training Loss: 0.280
Epoch 4/15 ; k=50 ; Training Loss: 0.376
Epoch 4/15 ; k=60 ; Training Loss: 0.105
Epoch 4/15 ; k=70 ; Training Loss: 0.250
Epoch 4/15 ; k=80 ; Training Loss: 0.274
Epoch 4/15 ; k=90 ; Training Loss: 0.152
Epoch 4/15 ; k=100 ; Training Loss: 0.242
Epoch 4/15 ; k=110 ; Training Loss: 0.322
Epoch 4/15 ; k=120 ; Training Loss: 0.143
Epoch 4/15 ; k=130 ; Training Loss: 0.118
Epoch 4/15 ; k=140 ; Training Loss: 0.141
Epoch 4/15 ; k=150 ; Training Loss: 0.063
Epoch 4/15 ; k=160 ; Training Loss: 0.206
Epoch 4/15 ; k=170 ; Training Loss: 0.163
Epoch 4/15 ; k=180 ; Training Loss: 0.167
Epoch 4/15 ; k=190 ; Training Loss: 0.073
Epoch 4/15 ; k=200 ; Training Loss: 0.209
Epoch 4/15 ; k=210 ; Training Loss: 0.228
Epoch 4/15 ; k=220 ; Training Loss: 0.219
Epoch 4/15 ; k=230 ; Training Loss: 0.198
Epoch 4/15 ; k=240 ; Training Loss: 0.114
Epoch 4/15 ; k=250 ; Training Loss: 0.106
Epoch 4/15 ; k=260 ; Training Loss: 0.113
Epoch 4/15 ; k=270 ; Training Loss: 0.123
Epoch 4/15 ; k=280 ; Training Loss: 0.102
Epoch 5/15 ; k=0 ; Training Loss: 0.095
Epoch 5/15 ; k=10 ; Training Loss: 0.105
Epoch 5/15 ; k=20 ; Training Loss: 0.103
Epoch 5/15 ; k=30 ; Training Loss: 0.220
Epoch 5/15 ; k=40 ; Training Loss: 0.119
Epoch 5/15 ; k=50 ; Training Loss: 0.097
Epoch 5/15 ; k=60 ; Training Loss: 0.103
Epoch 5/15 ; k=70 ; Training Loss: 0.090
Epoch 5/15 ; k=80 ; Training Loss: 0.097
Epoch 5/15 ; k=90 ; Training Loss: 0.278
Epoch 5/15 ; k=100 ; Training Loss: 0.136
Epoch 5/15 ; k=110 ; Training Loss: 0.236
Epoch 5/15 ; k=120 ; Training Loss: 0.212
Epoch 5/15 ; k=130 ; Training Loss: 0.134
Epoch 5/15 ; k=140 ; Training Loss: 0.243
Epoch 5/15 ; k=150 ; Training Loss: 0.046
Epoch 5/15 ; k=160 ; Training Loss: 0.171
Epoch 5/15 ; k=170 ; Training Loss: 0.078
Epoch 5/15 ; k=180 ; Training Loss: 0.150
Epoch 5/15 ; k=190 ; Training Loss: 0.076
Epoch 5/15 ; k=200 ; Training Loss: 0.134
Epoch 5/15 ; k=210 ; Training Loss: 0.128
Epoch 5/15 ; k=220 ; Training Loss: 0.105
Epoch 5/15 ; k=230 ; Training Loss: 0.097
Epoch 5/15 ; k=240 ; Training Loss: 0.141
Epoch 5/15 ; k=250 ; Training Loss: 0.141
Epoch 5/15 ; k=260 ; Training Loss: 0.094
Epoch 5/15 ; k=270 ; Training Loss: 0.115
Epoch 5/15 ; k=280 ; Training Loss: 0.100
Epoch 6/15 ; k=0 ; Training Loss: 0.218
Epoch 6/15 ; k=10 ; Training Loss: 0.123
Epoch 6/15 ; k=20 ; Training Loss: 0.075
Epoch 6/15 ; k=30 ; Training Loss: 0.088
Epoch 6/15 ; k=40 ; Training Loss: 0.079
Epoch 6/15 ; k=50 ; Training Loss: 0.117
Epoch 6/15 ; k=60 ; Training Loss: 0.154
Epoch 6/15 ; k=70 ; Training Loss: 0.089
Epoch 6/15 ; k=80 ; Training Loss: 0.050
Epoch 6/15 ; k=90 ; Training Loss: 0.080
Epoch 6/15 ; k=100 ; Training Loss: 0.068
Epoch 6/15 ; k=110 ; Training Loss: 0.088
Epoch 6/15 ; k=120 ; Training Loss: 0.055
Epoch 6/15 ; k=130 ; Training Loss: 0.109
Epoch 6/15 ; k=140 ; Training Loss: 0.102
Epoch 6/15 ; k=150 ; Training Loss: 0.129
Epoch 6/15 ; k=160 ; Training Loss: 0.079
Epoch 6/15 ; k=170 ; Training Loss: 0.113
Epoch 6/15 ; k=180 ; Training Loss: 0.145
Epoch 6/15 ; k=190 ; Training Loss: 0.067
Epoch 6/15 ; k=200 ; Training Loss: 0.031
Epoch 6/15 ; k=210 ; Training Loss: 0.102
Epoch 6/15 ; k=220 ; Training Loss: 0.131
Epoch 6/15 ; k=230 ; Training Loss: 0.058
Epoch 6/15 ; k=240 ; Training Loss: 0.124
Epoch 6/15 ; k=250 ; Training Loss: 0.048
Epoch 6/15 ; k=260 ; Training Loss: 0.303
Epoch 6/15 ; k=270 ; Training Loss: 0.061
Epoch 6/15 ; k=280 ; Training Loss: 0.140
Epoch 7/15 ; k=0 ; Training Loss: 0.116
Epoch 7/15 ; k=10 ; Training Loss: 0.107
Epoch 7/15 ; k=20 ; Training Loss: 0.043
Epoch 7/15 ; k=30 ; Training Loss: 0.094
Epoch 7/15 ; k=40 ; Training Loss: 0.061
Epoch 7/15 ; k=50 ; Training Loss: 0.125
Epoch 7/15 ; k=60 ; Training Loss: 0.110
Epoch 7/15 ; k=70 ; Training Loss: 0.104
Epoch 7/15 ; k=80 ; Training Loss: 0.158
Epoch 7/15 ; k=90 ; Training Loss: 0.153
Epoch 7/15 ; k=100 ; Training Loss: 0.122
Epoch 7/15 ; k=110 ; Training Loss: 0.181
Epoch 7/15 ; k=120 ; Training Loss: 0.076
Epoch 7/15 ; k=130 ; Training Loss: 0.195
Epoch 7/15 ; k=140 ; Training Loss: 0.071
Epoch 7/15 ; k=150 ; Training Loss: 0.088
Epoch 7/15 ; k=160 ; Training Loss: 0.134
Epoch 7/15 ; k=170 ; Training Loss: 0.089
Epoch 7/15 ; k=180 ; Training Loss: 0.171
Epoch 7/15 ; k=190 ; Training Loss: 0.075
Epoch 7/15 ; k=200 ; Training Loss: 0.081
Epoch 7/15 ; k=210 ; Training Loss: 0.073
Epoch 7/15 ; k=220 ; Training Loss: 0.105
Epoch 7/15 ; k=230 ; Training Loss: 0.058
Epoch 7/15 ; k=240 ; Training Loss: 0.067
Epoch 7/15 ; k=250 ; Training Loss: 0.087
Epoch 7/15 ; k=260 ; Training Loss: 0.092
Epoch 7/15 ; k=270 ; Training Loss: 0.056
Epoch 7/15 ; k=280 ; Training Loss: 0.097
Epoch 8/15 ; k=0 ; Training Loss: 0.070
Epoch 8/15 ; k=10 ; Training Loss: 0.111
Epoch 8/15 ; k=20 ; Training Loss: 0.094
Epoch 8/15 ; k=30 ; Training Loss: 0.090
Epoch 8/15 ; k=40 ; Training Loss: 0.099
Epoch 8/15 ; k=50 ; Training Loss: 0.042
Epoch 8/15 ; k=60 ; Training Loss: 0.104
Epoch 8/15 ; k=70 ; Training Loss: 0.101
Epoch 8/15 ; k=80 ; Training Loss: 0.136
Epoch 8/15 ; k=90 ; Training Loss: 0.100
Epoch 8/15 ; k=100 ; Training Loss: 0.055
Epoch 8/15 ; k=110 ; Training Loss: 0.050
Epoch 8/15 ; k=120 ; Training Loss: 0.065
Epoch 8/15 ; k=130 ; Training Loss: 0.238
Epoch 8/15 ; k=140 ; Training Loss: 0.181
Epoch 8/15 ; k=150 ; Training Loss: 0.233
Epoch 8/15 ; k=160 ; Training Loss: 0.113
Epoch 8/15 ; k=170 ; Training Loss: 0.085
Epoch 8/15 ; k=180 ; Training Loss: 0.067
Epoch 8/15 ; k=190 ; Training Loss: 0.110
Epoch 8/15 ; k=200 ; Training Loss: 0.089
Epoch 8/15 ; k=210 ; Training Loss: 0.090
Epoch 8/15 ; k=220 ; Training Loss: 0.035
Epoch 8/15 ; k=230 ; Training Loss: 0.160
Epoch 8/15 ; k=240 ; Training Loss: 0.037
Epoch 8/15 ; k=250 ; Training Loss: 0.069
Epoch 8/15 ; k=260 ; Training Loss: 0.080
Epoch 8/15 ; k=270 ; Training Loss: 0.059
Epoch 8/15 ; k=280 ; Training Loss: 0.098
Epoch 9/15 ; k=0 ; Training Loss: 0.068
Epoch 9/15 ; k=10 ; Training Loss: 0.086
Epoch 9/15 ; k=20 ; Training Loss: 0.064
Epoch 9/15 ; k=30 ; Training Loss: 0.074
Epoch 9/15 ; k=40 ; Training Loss: 0.088
Epoch 9/15 ; k=50 ; Training Loss: 0.098
Epoch 9/15 ; k=60 ; Training Loss: 0.052
Epoch 9/15 ; k=70 ; Training Loss: 0.091
Epoch 9/15 ; k=80 ; Training Loss: 0.121
Epoch 9/15 ; k=90 ; Training Loss: 0.047
Epoch 9/15 ; k=100 ; Training Loss: 0.044
Epoch 9/15 ; k=110 ; Training Loss: 0.040
Epoch 9/15 ; k=120 ; Training Loss: 0.083
Epoch 9/15 ; k=130 ; Training Loss: 0.083
Epoch 9/15 ; k=140 ; Training Loss: 0.063
Epoch 9/15 ; k=150 ; Training Loss: 0.077
Epoch 9/15 ; k=160 ; Training Loss: 0.089
Epoch 9/15 ; k=170 ; Training Loss: 0.148
Epoch 9/15 ; k=180 ; Training Loss: 0.129
Epoch 9/15 ; k=190 ; Training Loss: 0.074
Epoch 9/15 ; k=200 ; Training Loss: 0.096
Epoch 9/15 ; k=210 ; Training Loss: 0.037
Epoch 9/15 ; k=220 ; Training Loss: 0.105
Epoch 9/15 ; k=230 ; Training Loss: 0.050
Epoch 9/15 ; k=240 ; Training Loss: 0.058
Epoch 9/15 ; k=250 ; Training Loss: 0.081
Epoch 9/15 ; k=260 ; Training Loss: 0.088
Epoch 9/15 ; k=270 ; Training Loss: 0.104
Epoch 9/15 ; k=280 ; Training Loss: 0.064
Epoch 10/15 ; k=0 ; Training Loss: 0.087
Epoch 10/15 ; k=10 ; Training Loss: 0.066
Epoch 10/15 ; k=20 ; Training Loss: 0.088
Epoch 10/15 ; k=30 ; Training Loss: 0.112
Epoch 10/15 ; k=40 ; Training Loss: 0.069
Epoch 10/15 ; k=50 ; Training Loss: 0.036
Epoch 10/15 ; k=60 ; Training Loss: 0.085
Epoch 10/15 ; k=70 ; Training Loss: 0.059
Epoch 10/15 ; k=80 ; Training Loss: 0.061
Epoch 10/15 ; k=90 ; Training Loss: 0.060
Epoch 10/15 ; k=100 ; Training Loss: 0.101
Epoch 10/15 ; k=110 ; Training Loss: 0.053
Epoch 10/15 ; k=120 ; Training Loss: 0.063
Epoch 10/15 ; k=130 ; Training Loss: 0.186
Epoch 10/15 ; k=140 ; Training Loss: 0.102
Epoch 10/15 ; k=150 ; Training Loss: 0.094
Epoch 10/15 ; k=160 ; Training Loss: 0.119
Epoch 10/15 ; k=170 ; Training Loss: 0.060
Epoch 10/15 ; k=180 ; Training Loss: 0.053
Epoch 10/15 ; k=190 ; Training Loss: 0.278
Epoch 10/15 ; k=200 ; Training Loss: 0.087
Epoch 10/15 ; k=210 ; Training Loss: 0.107
Epoch 10/15 ; k=220 ; Training Loss: 0.172
Epoch 10/15 ; k=230 ; Training Loss: 0.151
Epoch 10/15 ; k=240 ; Training Loss: 0.167
Epoch 10/15 ; k=250 ; Training Loss: 0.079
Epoch 10/15 ; k=260 ; Training Loss: 0.089
Epoch 10/15 ; k=270 ; Training Loss: 0.079
Epoch 10/15 ; k=280 ; Training Loss: 0.179
Gear Shift of the learning_rate
Epoch 11/15 ; k=0 ; Training Loss: 0.100
Epoch 11/15 ; k=10 ; Training Loss: 0.056
Epoch 11/15 ; k=20 ; Training Loss: 0.036
Epoch 11/15 ; k=30 ; Training Loss: 0.094
Epoch 11/15 ; k=40 ; Training Loss: 0.045
Epoch 11/15 ; k=50 ; Training Loss: 0.035
Epoch 11/15 ; k=60 ; Training Loss: 0.103
Epoch 11/15 ; k=70 ; Training Loss: 0.060
Epoch 11/15 ; k=80 ; Training Loss: 0.082
Epoch 11/15 ; k=90 ; Training Loss: 0.051
Epoch 11/15 ; k=100 ; Training Loss: 0.072
Epoch 11/15 ; k=110 ; Training Loss: 0.128
Epoch 11/15 ; k=120 ; Training Loss: 0.021
Epoch 11/15 ; k=130 ; Training Loss: 0.052
Epoch 11/15 ; k=140 ; Training Loss: 0.082
Epoch 11/15 ; k=150 ; Training Loss: 0.111
Epoch 11/15 ; k=160 ; Training Loss: 0.101
Epoch 11/15 ; k=170 ; Training Loss: 0.067
Epoch 11/15 ; k=180 ; Training Loss: 0.138
Epoch 11/15 ; k=190 ; Training Loss: 0.111
Epoch 11/15 ; k=200 ; Training Loss: 0.045
Epoch 11/15 ; k=210 ; Training Loss: 0.034
Epoch 11/15 ; k=220 ; Training Loss: 0.041
Epoch 11/15 ; k=230 ; Training Loss: 0.042
Epoch 11/15 ; k=240 ; Training Loss: 0.127
Epoch 11/15 ; k=250 ; Training Loss: 0.049
Epoch 11/15 ; k=260 ; Training Loss: 0.048
Epoch 11/15 ; k=270 ; Training Loss: 0.162
Epoch 11/15 ; k=280 ; Training Loss: 0.053
Epoch 12/15 ; k=0 ; Training Loss: 0.114
Epoch 12/15 ; k=10 ; Training Loss: 0.064
Epoch 12/15 ; k=20 ; Training Loss: 0.101
Epoch 12/15 ; k=30 ; Training Loss: 0.085
Epoch 12/15 ; k=40 ; Training Loss: 0.086
Epoch 12/15 ; k=50 ; Training Loss: 0.051
Epoch 12/15 ; k=60 ; Training Loss: 0.035
Epoch 12/15 ; k=70 ; Training Loss: 0.118
Epoch 12/15 ; k=80 ; Training Loss: 0.102
Epoch 12/15 ; k=90 ; Training Loss: 0.068
Epoch 12/15 ; k=100 ; Training Loss: 0.101
Epoch 12/15 ; k=110 ; Training Loss: 0.158
Epoch 12/15 ; k=120 ; Training Loss: 0.092
Epoch 12/15 ; k=130 ; Training Loss: 0.117
Epoch 12/15 ; k=140 ; Training Loss: 0.035
Epoch 12/15 ; k=150 ; Training Loss: 0.042
Epoch 12/15 ; k=160 ; Training Loss: 0.083
Epoch 12/15 ; k=170 ; Training Loss: 0.054
Epoch 12/15 ; k=180 ; Training Loss: 0.041
Epoch 12/15 ; k=190 ; Training Loss: 0.085
Epoch 12/15 ; k=200 ; Training Loss: 0.116
Epoch 12/15 ; k=210 ; Training Loss: 0.077
Epoch 12/15 ; k=220 ; Training Loss: 0.099
Epoch 12/15 ; k=230 ; Training Loss: 0.045
Epoch 12/15 ; k=240 ; Training Loss: 0.062
Epoch 12/15 ; k=250 ; Training Loss: 0.059
Epoch 12/15 ; k=260 ; Training Loss: 0.059
Epoch 12/15 ; k=270 ; Training Loss: 0.025
Epoch 12/15 ; k=280 ; Training Loss: 0.046
Epoch 13/15 ; k=0 ; Training Loss: 0.082
Epoch 13/15 ; k=10 ; Training Loss: 0.066
Epoch 13/15 ; k=20 ; Training Loss: 0.046
Epoch 13/15 ; k=30 ; Training Loss: 0.101
Epoch 13/15 ; k=40 ; Training Loss: 0.069
Epoch 13/15 ; k=50 ; Training Loss: 0.059
Epoch 13/15 ; k=60 ; Training Loss: 0.060
Epoch 13/15 ; k=70 ; Training Loss: 0.061
Epoch 13/15 ; k=80 ; Training Loss: 0.041
Epoch 13/15 ; k=90 ; Training Loss: 0.044
Epoch 13/15 ; k=100 ; Training Loss: 0.034
Epoch 13/15 ; k=110 ; Training Loss: 0.065
Epoch 13/15 ; k=120 ; Training Loss: 0.036
Epoch 13/15 ; k=130 ; Training Loss: 0.040
Epoch 13/15 ; k=140 ; Training Loss: 0.058
Epoch 13/15 ; k=150 ; Training Loss: 0.159
Epoch 13/15 ; k=160 ; Training Loss: 0.056
Epoch 13/15 ; k=170 ; Training Loss: 0.194
Epoch 13/15 ; k=180 ; Training Loss: 0.055
Epoch 13/15 ; k=190 ; Training Loss: 0.034
Epoch 13/15 ; k=200 ; Training Loss: 0.026
Epoch 13/15 ; k=210 ; Training Loss: 0.068
Epoch 13/15 ; k=220 ; Training Loss: 0.090
Epoch 13/15 ; k=230 ; Training Loss: 0.034
Epoch 13/15 ; k=240 ; Training Loss: 0.096
Epoch 13/15 ; k=250 ; Training Loss: 0.024
Epoch 13/15 ; k=260 ; Training Loss: 0.037
Epoch 13/15 ; k=270 ; Training Loss: 0.030
Epoch 13/15 ; k=280 ; Training Loss: 0.058
Epoch 14/15 ; k=0 ; Training Loss: 0.049
Epoch 14/15 ; k=10 ; Training Loss: 0.066
Epoch 14/15 ; k=20 ; Training Loss: 0.046
Epoch 14/15 ; k=30 ; Training Loss: 0.109
Epoch 14/15 ; k=40 ; Training Loss: 0.051
Epoch 14/15 ; k=50 ; Training Loss: 0.075
Epoch 14/15 ; k=60 ; Training Loss: 0.057
Epoch 14/15 ; k=70 ; Training Loss: 0.026
Epoch 14/15 ; k=80 ; Training Loss: 0.083
Epoch 14/15 ; k=90 ; Training Loss: 0.059
Epoch 14/15 ; k=100 ; Training Loss: 0.136
Epoch 14/15 ; k=110 ; Training Loss: 0.045
Epoch 14/15 ; k=120 ; Training Loss: 0.056
Epoch 14/15 ; k=130 ; Training Loss: 0.073
Epoch 14/15 ; k=140 ; Training Loss: 0.089
Epoch 14/15 ; k=150 ; Training Loss: 0.069
Epoch 14/15 ; k=160 ; Training Loss: 0.032
Epoch 14/15 ; k=170 ; Training Loss: 0.049
Epoch 14/15 ; k=180 ; Training Loss: 0.056
Epoch 14/15 ; k=190 ; Training Loss: 0.072
Epoch 14/15 ; k=200 ; Training Loss: 0.114
Epoch 14/15 ; k=210 ; Training Loss: 0.045
Epoch 14/15 ; k=220 ; Training Loss: 0.060
Epoch 14/15 ; k=230 ; Training Loss: 0.032
Epoch 14/15 ; k=240 ; Training Loss: 0.027
Epoch 14/15 ; k=250 ; Training Loss: 0.035
Epoch 14/15 ; k=260 ; Training Loss: 0.058
Epoch 14/15 ; k=270 ; Training Loss: 0.048
Epoch 14/15 ; k=280 ; Training Loss: 0.025
Epoch 15/15 ; k=0 ; Training Loss: 0.085
Epoch 15/15 ; k=10 ; Training Loss: 0.036
Epoch 15/15 ; k=20 ; Training Loss: 0.025
Epoch 15/15 ; k=30 ; Training Loss: 0.105
Epoch 15/15 ; k=40 ; Training Loss: 0.025
Epoch 15/15 ; k=50 ; Training Loss: 0.052
Epoch 15/15 ; k=60 ; Training Loss: 0.034
Epoch 15/15 ; k=70 ; Training Loss: 0.082
Epoch 15/15 ; k=80 ; Training Loss: 0.059
Epoch 15/15 ; k=90 ; Training Loss: 0.055
Epoch 15/15 ; k=100 ; Training Loss: 0.061
Epoch 15/15 ; k=110 ; Training Loss: 0.060
Epoch 15/15 ; k=120 ; Training Loss: 0.045
Epoch 15/15 ; k=130 ; Training Loss: 0.042
Epoch 15/15 ; k=140 ; Training Loss: 0.044
Epoch 15/15 ; k=150 ; Training Loss: 0.054
Epoch 15/15 ; k=160 ; Training Loss: 0.062
Epoch 15/15 ; k=170 ; Training Loss: 0.054
Epoch 15/15 ; k=180 ; Training Loss: 0.030
Epoch 15/15 ; k=190 ; Training Loss: 0.059
Epoch 15/15 ; k=200 ; Training Loss: 0.032
Epoch 15/15 ; k=210 ; Training Loss: 0.082
Epoch 15/15 ; k=220 ; Training Loss: 0.104
Epoch 15/15 ; k=230 ; Training Loss: 0.054
Epoch 15/15 ; k=240 ; Training Loss: 0.064
Epoch 15/15 ; k=250 ; Training Loss: 0.054
Epoch 15/15 ; k=260 ; Training Loss: 0.054
Epoch 15/15 ; k=270 ; Training Loss: 0.057
Epoch 15/15 ; k=280 ; Training Loss: 0.026
Training Finished. Saving test images to: ./runs/1502708866.5839972